// Copyright 2021 Adam Chalkley
//
// https://github.com/atc0005/check-vmware
//
// Licensed under the MIT License. See LICENSE file in the project root for
// full license information.

package main

import (
	"context"
	"errors"
	"fmt"
	"strings"

	"github.com/atc0005/go-nagios"
	"github.com/vmware/govmomi/vim25/mo"

	"github.com/atc0005/check-vmware/internal/config"
	"github.com/atc0005/check-vmware/internal/vsphere"

	zlog "github.com/rs/zerolog/log"
)

//go:generate go-winres make --product-version=git-tag --file-version=git-tag

func main() {

	plugin := nagios.NewPlugin()

	// defer this from the start so it is the last deferred function to run
	defer plugin.ReturnCheckResults()

	// Collect last minute details just before ending plugin execution.
	defer func(plugin *nagios.Plugin) {
		// Annotate errors (if applicable) with additional context to aid in
		// troubleshooting.
		plugin.Errors = vsphere.AnnotateError(plugin.Errors...)
	}(plugin)

	// Setup configuration by parsing user-provided flags. Note plugin type so
	// that only applicable CLI flags are exposed and any plugin-specific
	// settings are applied.
	cfg, cfgErr := config.New(config.PluginType{DiskConsolidation: true})
	switch {
	case errors.Is(cfgErr, config.ErrVersionRequested):
		fmt.Println(config.Version())

		return

	case cfgErr != nil:
		// We're using the standalone Err function from rs/zerolog/log as we
		// do not have a working configuration.
		zlog.Err(cfgErr).Msg("Error initializing application")
		plugin.ServiceOutput = fmt.Sprintf(
			"%s: Error initializing application",
			nagios.StateCRITICALLabel,
		)
		plugin.AddError(cfgErr)
		plugin.ExitStatusCode = nagios.StateCRITICALExitCode

		return
	}

	// Enable library-level logging if debug or greater logging level is
	// enabled app-wide.
	handleLibraryLogging()

	// Set context deadline equal to user-specified timeout value for plugin
	// runtime/execution.
	ctx, cancel := context.WithTimeout(context.Background(), cfg.Timeout())
	defer cancel()

	// Record thresholds for use as Nagios "Long Service Output" content. This
	// content is shown in the detailed web UI and in notifications generated
	// by Nagios.
	plugin.CriticalThreshold = "Disk consolidation needed for one or more Virtual Machines."

	plugin.WarningThreshold = config.ThresholdNotUsed

	if cfg.EmitBranding {
		// If enabled, show application details at end of notification
		plugin.BrandingCallback = config.Branding("Notification generated by ")
	}

	log := cfg.Log.With().
		Str("included_resource_pools", cfg.IncludedResourcePools.String()).
		Str("excluded_resource_pools", cfg.ExcludedResourcePools.String()).
		Str("ignored_vms", cfg.IgnoredVMs.String()).
		Logger()

	log.Debug().Msg("Logging into vSphere environment")
	c, loginErr := vsphere.Login(
		ctx, cfg.Server, cfg.Port, cfg.TrustCert,
		cfg.Username, cfg.Domain, cfg.Password,
		cfg.UserAgent(),
	)
	if loginErr != nil {
		log.Error().Err(loginErr).Msgf("error logging into %s", cfg.Server)

		plugin.AddError(loginErr)
		plugin.ServiceOutput = fmt.Sprintf(
			"%s: Error logging into %q",
			nagios.StateCRITICALLabel,
			cfg.Server,
		)
		plugin.ExitStatusCode = nagios.StateCRITICALExitCode

		return
	}
	log.Debug().Msg("Successfully logged into vSphere environment")

	defer func() {
		if err := c.Logout(ctx); err != nil {
			log.Error().
				Err(err).
				Msg("failed to logout")
		}
	}()

	// At this point we're logged in, ready to retrieve a list of VMs. If
	// specified, we should limit VMs based on include/exclude lists. First,
	// we'll make sure that all specified resource pools actually exist in the
	// vSphere environment.

	log.Debug().Msg("Validating resource pools")
	validateErr := vsphere.ValidateRPs(ctx, c.Client, cfg.IncludedResourcePools, cfg.ExcludedResourcePools)
	if validateErr != nil {
		log.Error().Err(validateErr).Msg("error validating include/exclude lists")

		plugin.AddError(validateErr)
		plugin.ServiceOutput = fmt.Sprintf(
			"%s: Error validating include/exclude lists",
			nagios.StateCRITICALLabel,
		)
		plugin.ExitStatusCode = nagios.StateCRITICALExitCode

		return
	}

	log.Debug().Msg("Retrieving eligible resource pools")
	resourcePools, getRPsErr := vsphere.GetEligibleRPs(
		ctx,
		c.Client,
		cfg.IncludedResourcePools,
		cfg.ExcludedResourcePools,
		true,
	)
	if getRPsErr != nil {
		log.Error().Err(getRPsErr).Msg(
			"error retrieving list of resource pools",
		)

		plugin.AddError(getRPsErr)
		plugin.ServiceOutput = fmt.Sprintf(
			"%s: Error retrieving list of resource pools from %q",
			nagios.StateCRITICALLabel,
			cfg.Server,
		)
		plugin.ExitStatusCode = nagios.StateCRITICALExitCode

		return
	}

	rpNames := make([]string, 0, len(resourcePools))
	for _, rp := range resourcePools {
		rpNames = append(rpNames, rp.Name)
	}

	log.Debug().
		Str("resource_pools", strings.Join(rpNames, ", ")).
		Msg("")

	log.Debug().Msg("Retrieving vms from eligible resource pools")
	rpEntityVals := make([]mo.ManagedEntity, 0, len(resourcePools))
	for i := range resourcePools {
		rpEntityVals = append(rpEntityVals, resourcePools[i].ManagedEntity)
	}
	vms, getVMsErr := vsphere.GetVMsFromContainer(ctx, c.Client, true, rpEntityVals...)
	if getVMsErr != nil {
		log.Error().Err(getVMsErr).Msg(
			"error retrieving list of VMs from resource pools list",
		)

		plugin.AddError(getVMsErr)
		plugin.ServiceOutput = fmt.Sprintf(
			"%s: Error retrieving list of VMs from resource pools list",
			nagios.StateCRITICALLabel,
		)
		plugin.ExitStatusCode = nagios.StateCRITICALExitCode

		return
	}

	log.Debug().
		Str("vms_evaluated", strings.Join(vsphere.VMNames(vms), ", ")).
		Msg("Evaluated Virtual Machines")

	log.Debug().Msg("Drop any VMs we've been asked to exclude from checks")
	filteredVMs, numVMsExcludedByName := vsphere.ExcludeVMsByName(vms, cfg.IgnoredVMs)

	log.Debug().
		Str("vms_filtered_by_name", strings.Join(vsphere.VMNames(filteredVMs), ", ")).
		Int("vms_excluded_by_name", numVMsExcludedByName).
		Msg("VMs after name filtering")

	// NOTE: This plugin is hard-coded to evaluate powered off and powered
	// on VMs equally. I'm not sure whether ignoring powered off VMs by
	// default makes sense for this particular plugin.
	//
	// Please share your feedback here if you feel differently:
	// https://github.com/atc0005/check-vmware/discussions/176
	//
	// Please expand on some use cases for ignoring powered off VMs by default.
	//
	// 	log.Debug().Msg("Filter VMs to specified power state")
	// 	filteredVMs, numVMsExcludedByPowerState := vsphere.FilterVMsByPowerState(filteredVMs, cfg.PoweredOff)
	//
	// 	log.Debug().
	// 		Str("vms_filtered_by_power_state", strings.Join(vsphere.VMNames(filteredVMs), ", ")).
	// 		Int("vms_excluded_by_power_state", numVMsExcludedByPowerState).
	// 		Msg("VMs after power state filtering")

	// here we diverge from other plugins

	// State reload/refresh operation for remaining VMs is potentially
	// expensive, so only perform this step if requested.
	switch {
	case cfg.TriggerReloadStateData:
		log.Debug().
			Bool("trigger_state_reload", cfg.TriggerReloadStateData).
			Msg("Triggering reload of each VirtualMachine to ensure fresh metadata")
		vmEntityVals := make([]mo.ManagedEntity, 0, len(filteredVMs))
		for i := range filteredVMs {
			vmEntityVals = append(vmEntityVals, filteredVMs[i].ManagedEntity)
		}
		if err := vsphere.TriggerEntityStateReload(ctx, c.Client, vmEntityVals...); err != nil {
			log.Error().Err(err).Msg(
				"error triggering state reload for VMs",
			)

			plugin.AddError(err)
			plugin.ServiceOutput = fmt.Sprintf(
				"%s: Error triggering state reload for VMs",
				nagios.StateCRITICALLabel,
			)
			plugin.ExitStatusCode = nagios.StateCRITICALExitCode

			return
		}

	default:
		log.Debug().
			Bool("trigger_state_reload", cfg.TriggerReloadStateData).
			Msg("Trigger reload flag not specified, skipping reload/refresh of VirtualMachine state data")

	}

	log.Debug().Msg("Filter VMs to those needing disk consolidation")
	// Create a new collection of VMs with just those found to require disk
	// consolidation. Keep filteredVMs collection as-is; we'll use that as our
	// "baseline" against the list of VMs found requiring disk consolidation.
	vmsNeedingConsolidation, numVMsExcludedByConsolidationState := vsphere.FilterVMsByDiskConsolidationState(filteredVMs)
	numVMsRequiringDiskConsolidation := len(vmsNeedingConsolidation)

	log.Debug().
		Str("vms_filtered_by_disk_consolidation_status", strings.Join(vsphere.VMNames(vmsNeedingConsolidation), ", ")).
		Int("vms_needing_consolidation", numVMsRequiringDiskConsolidation).
		Int("vms_excluded_by_consolidation_state", numVMsExcludedByConsolidationState).
		Msg("VMs after disk consolidation needed filtering")

	log.Debug().Msg("Compiling Performance Data details")

	pd := []nagios.PerformanceData{
		// The `time` (runtime) metric is appended at plugin exit, so do not
		// duplicate it here.
		{
			Label: "vms",
			Value: fmt.Sprintf("%d", len(vms)),
		},
		{
			Label: "vms_excluded_by_name",
			Value: fmt.Sprintf("%d", numVMsExcludedByName),
		},
		{
			Label: "vms_with_consolidation_need",
			Value: fmt.Sprintf("%d", numVMsRequiringDiskConsolidation),
		},
		{
			Label: "vms_without_consolidation_need",
			Value: fmt.Sprintf("%d", numVMsExcludedByConsolidationState),
		},
		{
			Label: "resource_pools_excluded",
			Value: fmt.Sprintf("%d", len(cfg.ExcludedResourcePools)),
		},
		{
			Label: "resource_pools_included",
			Value: fmt.Sprintf("%d", len(cfg.IncludedResourcePools)),
		},
		{
			Label: "resource_pools_evaluated",
			Value: fmt.Sprintf("%d", len(resourcePools)),
		},
	}

	// Update logger with new performance data related fields
	log = log.With().
		Int("vms_total", len(vms)).
		Int("vms_filtered", len(filteredVMs)).
		Int("vms_excluded_by_name", numVMsExcludedByName).
		Int("vms_with_consolidation_need", numVMsRequiringDiskConsolidation).
		Int("vms_without_consolidation_need", numVMsExcludedByConsolidationState).
		Int("resource_pools_evaluated", len(resourcePools)).
		Logger()

	switch {
	case numVMsRequiringDiskConsolidation > 0:

		// *ANY* VMs requiring disk consolidation results in a CRITICAL state.

		vmsList := strings.Join(vsphere.VMNames(vmsNeedingConsolidation), ", ")

		log.Error().
			Str("virtual_machines", vmsList).
			Msg("Virtual Machines found in need of disk consolidation")

		plugin.AddError(vsphere.ErrVirtualMachineDiskConsolidationNeeded)

		plugin.ServiceOutput = vsphere.VMDiskConsolidationOneLineCheckSummary(
			nagios.StateCRITICALLabel,
			filteredVMs,
			vmsNeedingConsolidation,
			resourcePools,
		)

		plugin.LongServiceOutput = vsphere.VMDiskConsolidationReport(
			c.Client,
			vms,
			filteredVMs,
			vmsNeedingConsolidation,
			cfg.IgnoredVMs,
			cfg.IncludedResourcePools,
			cfg.ExcludedResourcePools,
			resourcePools,
		)

		if err := plugin.AddPerfData(false, pd...); err != nil {
			log.Error().
				Err(err).
				Msg("failed to add performance data")
		}

		plugin.ExitStatusCode = nagios.StateCRITICALExitCode

		return

	default:

		// success path

		log.Debug().Msg("VirtualMachine disk consolidation not needed")

		plugin.ServiceOutput = vsphere.VMDiskConsolidationOneLineCheckSummary(
			nagios.StateOKLabel,
			filteredVMs,
			vmsNeedingConsolidation,
			resourcePools,
		)

		plugin.LongServiceOutput = vsphere.VMDiskConsolidationReport(
			c.Client,
			vms,
			filteredVMs,
			vmsNeedingConsolidation,
			cfg.IgnoredVMs,
			cfg.IncludedResourcePools,
			cfg.ExcludedResourcePools,
			resourcePools,
		)

		if err := plugin.AddPerfData(false, pd...); err != nil {
			log.Error().
				Err(err).
				Msg("failed to add performance data")
		}

		plugin.ExitStatusCode = nagios.StateOKExitCode

		return
	}

}
